---
title: "ScPoEconometrics"
subtitle: "Session 3"
author: "Pierre Villedieu"
date: "SciencesPo Paris </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---

layout: true

<div class="my-footer"><img src="../img/logo/ScPo-shield.png" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

# define vars
om = par("mar")
lowtop = c(om[1],om[2],0.1,om[4])
library(magrittr)
library(repmis)
library(ggplot2)
```

# Recap from from past Lectures and plan for this session

Plan 

* Introduction to Simple Linear Regression Model

* Study the relation between **class size** and **students performance**

---

# Data

Student performance and class size 

.pull-left[
* Let's first download the data from [here](https://www.dropbox.com/s/2g16pid3eky44bp/grades_classize.RData?dl=0). ADD SOURCE
```{r, echo=FALSE}
grades = read.csv2("https://www.dropbox.com/s/jrvjrdcly0igbed/grades_classize.csv?dl=1")
```

* What are the variables? What is an observation here? 

* How the average math score (`av_grade`) is related to class size (`classize`)? 

* What is a good summary of their relationship?

]

--

.pull-right[
* Let's plot these two variables together. 

```{r,echo=FALSE,fig.align='center',fig.height=7,fig.width=8}
plot(av_grade ~ classsize, data = grades,
     xlab = "Class size (classsize)",
     ylab = "Average math grade (av_grade)",
     main = "Average math score vs class size ",
     pch  = 20,
     cex  = 2,
     col  = "red")
```
]

---

# A Line throught the Scatterplot

.left-wide[
```{r,echo=FALSE,fig.align='center',fig.height=5,fig.width=7}
plot(av_grade ~ classsize, data = grades,
     xlab = "Class size (classsize)",
     ylab = "Average math grade (av_grade)",
     main = "Average math score vs class size ",
     pch  = 20,
     cex  = 2,
     col  = "red")
abline(a = 66,b = 0,lw=3)
```
]

--

.right-thin[
<br>
<br>

* A *line*! Great. But **which** line? This one?

* That's a *flat* line. But `av_grade` is increasing. 

* `r emo::ji("weary")`

]

---

# A Line throught the Scatterplot

.left-wide[
```{r,echo=FALSE,fig.align='center',fig.height=5,fig.width=7}
plot(av_grade ~ classsize, data = grades,
     xlab = "Class size (classsize)",
     ylab = "Average math grade (av_grade)",
     main = "Average math score vs class size ",
     pch  = 20,
     cex  = 2,
     col  = "red")
abline(a = 55,b = 0.4,lw=3)
```
]

--

.right-thin[
<br>
<br>

* **That** one?

* Slightly better. Has a *slope* and an *intercept*.

* `r emo::ji("neutral_face")`

]


---

# Writing Down A *Line*

.pull-left[
* We observe $(y_i,x_i)$ in the data.

* This describes a line with intercept $b_0$ and slope $b_1$:
    $$
    \widehat{y}_i = b\_0 + b\_1 x\_i
    $$

* We call $\widehat{y}_i$ the *prediction* for $y_i$.

* Most of the times, $\widehat{y}_i \neq y_i$, i.e. we make an *error*.
]

--

.pull-right[

* At point $x_i$ we make error $e_i$.

* Our aim will be to keep the error *as small as possible*, while at the same time giving a reasonable description of the data.

* (We could be more generally trying to fit a *curve* rather than a *line*, by the way.)

* The *actual data* $(y_i,x_i)$ can thus be written like *prediction + error*:
    $$
    y_i = b_0 + b_1 x_i + e_i
    $$
]



---

# Making Errors

```{r, echo = FALSE, message = FALSE, warning = FALSE}
generate_data = function(int = 0.5,
                         slope = 1,
                         sigma = 10,
                         n_obs = 9,
                         x_min = 0,
                         x_max = 10) {
  x = seq(x_min, x_max, length.out = n_obs)
  y = int + slope * x + rnorm(n_obs, 0, sigma)
  fit = lm(y ~ x)
  y_hat = fitted(fit)
  y_bar = rep(mean(y), n_obs)
  error = resid(fit)
  meandev = y - y_bar
  data.frame(x, y, y_hat, y_bar, error, meandev)
}

plot_total_dev = function(reg_data,title=NULL) {
  if (is.null(title)){
    plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 4, col = "black")
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$meandev), ytop = reg_data$y - reg_data$meandev, density = -1,
         col = rgb(red = 1, green = 0, blue = 0, alpha = 0.5), border = NA)
  } else {
    plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey",main=title,ylim=c(-2,10.5))
     axis(side=2,at=seq(-2,10,by=2))
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$meandev), ytop = reg_data$y - reg_data$meandev, density = -1,
         col = rgb(red = 1, green = 0, blue = 0, alpha = 0.5), border = NA)
  }
  # arrows(reg_data$x, reg_data$y_bar,
  #        reg_data$x, reg_data$y,
  #        col = 'grey', lwd = 1, lty = 3, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 4,col = "black")
  # abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_total_dev_prop = function(reg_data) {
  plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'dodgerblue', lwd = 1, lty = 2, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 2,col = "grey")
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_unexp_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 3,asp=1)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'red', lwd = 4, lty = 1, length = 0.1, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 4, col = "black")
}

plot_unexp_SSR = function(reg_data,asp=1,title=NULL) {
  if (is.null(title)){
      plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 4,
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$error), ytop = reg_data$y - reg_data$error, density = -1,
         col = rgb(red = 0, green = 1, blue = 0, alpha = 0.5), border = NA),asp=asp)
      abline(lm(y ~ x, data = reg_data), lwd = 4, col = "black")
  } else {
      plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2,
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$error), ytop = reg_data$y - reg_data$error, density = -1,
         col = rgb(red = 0, green = 1, blue = 0, alpha = 0.5), border = NA),asp=asp,main=title)
    axis(side=2,at=seq(-2,10,by=2))
      abline(lm(y ~ x, data = reg_data), lwd = 2, col = "black")
  }
}

plot_exp_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y, main = "SSReg (Sum of Squares Regression)",
  xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
  abline(h = mean(reg_data$y), col = "grey")
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(21)
plot_data = generate_data(sigma = 2)
```

.left-wide[
```{r line-arrows, echo=FALSE, message=FALSE, warning=FALSE, fig.align="center",fig.height=5.5,fig.width=8}
par(mar = lowtop)
plot_unexp_dev(plot_data)
par(mar = om)
```
]

.right-thin[
* Red Arrows are *errors* or *residuals* for each prediction.

* Often denoted $u$ or $e$.

* Note that we have both $e>0$ and $e<0$!
]

---
class: inverse

# App Time!

* Let's try to find the best line using only the *absolute value* of errors!

```{r,eval=FALSE}
library(ScPoEconometrics) # load our library
launchApp('reg_simple_arrows')
aboutApp('reg_simple_arrows') # explainer about app
```

---

# Writing Down *The Best* Line

.pull-left[
* choose $(b_0,b_1)$ s.t. the sum $e_1^2 + \dots + e_N^2$ is **as small as possible**

* $e_1^2 + \dots + e_N^2$ is the *sum of squared residuals*, or SSR.

* Wait a moment... Why *squared* residuals?!
]

--

.pull-right[

* In previous plot, errors of different sign $(+/-)$ cancel out!

* This makes it hard to find a good line.

* Squaring each $e_i$ solves that issue as $e_i^2 \geq 0, \forall i$.
]


---

# Best Line and Squared Errors

.left-wide[
```{r line-squares, echo=FALSE, message=FALSE, warning=FALSE,fig.width=8,fig.height = 5.5}
par(mar = lowtop)
plot_unexp_SSR(plot_data)
par(mar = om)
```
]

--

.right-thin[
<br>
<br>

* **That's**  the one!

* Perfect! Minimizes the sum of squares.

* `r emo::ji("relieved")`

]

---
class: inverse

# App Time!


```{r,eval=FALSE}
launchApp('reg_simple')
aboutApp('reg_simple')
```


---

# Ordinary Least Squares (OLS)

.pull-left[
* OLS estimates the best line for us.

* In our single regressor case, there is a simple formula for the slope:
  $$
  b_1 = \frac{cov(x,y)}{var(x)}
  $$
  
* and for the intercept
  $$
  b_0 = \bar{y} - b_1\bar{x}
  $$

* `r emo::ji("rotating_light")` You **must** know and understand those formulae!

]

--

.pull-right[

**Interpretation** (for now we assume x and y to be numerical). 

* Intercept $(b_0)$ : The predicted value of $y$ if $x$ is set to 0

* Slope $(b_1)$ : The change in $y$ associated to an increase of $x$ by one unit. 

]
  
---

# OLS : class size and average math score

Let's take back our grades-class size example! 

.pull-left[

```{r echo=FALSE, fig.width=5,fig.height = 4}
grades %>% 
        ggplot(aes(x=classsize,y=av_grade)) + 
        geom_point() + geom_smooth(method = "lm", se = F)

```
]

--

.pull-right[

* **OLS in `R`**. You can run an OLS regression by executing this command

```{r echo=T, eval = F}
reg = lm(av_grade~classsize,grades) # assign the result of the regression to the "reg" object 
summary(reg) # to view the output 
```


* We get theses values for $b_0$ and $b_1$

```{r echo=FALSE}
coeffs = lm(av_grade~classsize,grades)$coefficients
coeffs
```

* Interpret these two values.


]

---

class: inverse

# App Time!

How does OLS actually perform the minimization problem?

```{r,eval=FALSE}
launchApp('SSR_cone')
aboutApp('SSR_cone')  # after
```


---
class: inverse

# App Time!


Let's do some more OLS!

```{r,eval=FALSE}
launchApp('reg_full')
aboutApp('reg_full')  # after
```

---

# OLS without any Regressor

.pull-left[
* Our line is flat at level $b_0$:
  $$y = b_0$$
* Our optimization problem is now
  $$b_0 = \arg\min_{\text{int}} \sum_{i=1}^N \left[y_i - \text{int}\right]^2,$$
* With solution
  $$b_0 = \frac{1}{N} \sum_{i=1}^N y_i = \overline{y}.$$
]

--

.pull-right[
* In other words: Estimates the **mean** of $y$!

```{r,echo = FALSE, fig.height = 6}
par(mar = lowtop)
plot_total_dev(plot_data)
par(mar = om)
```
]


---

# Other OLS Restrictions

* There are other restrictions we can impose.

* They are described [in the book](https://scpoecon.github.io/ScPoEconometrics/linreg.html#OLS). Optional `r emo::ji("nerd_face")`.

* There is an app for each of them:
<br>
<br>

type | App  
-------- | --------
No Intercept | `launchApp('reg_constrained')` 
Centered Regression | `launchApp('demeaned_reg')` 
Standardized Regression | `launchApp('reg_standardized')`


---

# Predictions and Residuals


1. The error is $e_i = y_i - \widehat{y}_i$

2. The average of $\widehat{y}_i$ is equal to $\bar{y}$.
    $$\begin{align}\frac{1}{N} \sum_{i=1}^N \widehat{y}_i &= \frac{1}{N} \sum_{i=1}^N b_0 + b_1 x_i \\ &= b_0 + b_1  \bar{x}  = \bar{y} \end{align}$$
--
3. Then,
    $$\frac{1}{N} \sum_{i=1}^N e_i = \bar{y} - \frac{1}{N} \sum_{i=1}^N \widehat{y}_i = 0$$
    i.e. the average of errors is zero.

---


# Properties of Residuals

.pull-left[
1. The average of $\widehat{y}_i$ is the same as the mean of $y$.

2. The average of the errors should be zero.

3. Prediction and errors should be *uncorrelated* (i.e. orthogonal).

Let's look at the data behind our *arrows* plot above:

]

--

.pull-right[
To be fixed
```{r,echo=FALSE}
# sd = subset(plot_data,select=c(x,y,y_hat,error))
# sd = rbind(sd, colMeans(sd))
# sd = cbind(c(rep("",nrow(sd)-1),"Means"),sd)
# names(sd)[1] = ""
# ss = hux(sd)
# ss %>%
#     add_colnames() %>%
#     set_number_format(2) %>%
#     set_bottom_border(c(1,nrow(sd)), 2:5,2) %>%
#     set_align(row = 1,everywhere, "center")
```

]

---

# Properties of Residuals

.pull-left[

1. The average of $\widehat{y}_i$ is the same as the mean of $y$.

2. The average of the errors should be zero.

3. Prediction and errors should be *uncorrelated* (i.e. orthogonal).
]

--

.pull-right[
```{r}
# 1.
# all.equal(mean(sd$y_hat), mean(sd$y))

# 2.
# all.equal(mean(sd$error), 0)

# 3.
# all.equal(cov(sd$error,sd$y_hat), 0)
```
]

---


# Linear Statistics

* It's important to keep in mind that Var, Cov, Corr and Regression measure **linear relationships** between two variables.

* Two datasets with *identical* correlations could look *vastly* different.

* They would have the same regression line.

* Same correlation coefficient.

--

* Is that even possible?

---

# Linear Statistics: Anscombe

* Francis Anscombe (1973) comes up with 4 datasets with identical stats. But look!

.left-wide[

```{r,echo=FALSE,fig.height = 5}
##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
covs = data.frame(dataset = 1:4, cov = 0.0)
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  ## or   ff[[2]] <- as.name(paste0("y", i))
  ##      ff[[3]] <- as.name(paste0("x", i))
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
  covs[i,"cov"] = eval(parse(text = paste0("cov(anscombe$x",i,",anscombe$y",i,")")))
  covs[i,"var(y)"] = eval(parse(text = paste0("var(anscombe$y",i,")")))
  covs[i,"var(x)"] = eval(parse(text = paste0("var(anscombe$x",i,")")))
}

op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 0, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13),main=paste("dataset",i))
  abline(mods[[i]], col = "blue")
}
par(op)
```
]

--

.right-thin[
To be fixed 
```{r,echo = FALSE}
# ch = hux(covs)
# ch %>% 
#   set_number_format(row = everywhere, col = c(2:4), 2) %>%
#   set_number_format(row = everywhere, col = 1, 0) %>%
#   add_colnames()

```

]
---

# Dinosaurs in your Data?

* So, be wary of only looking a linear summary stats.
* Also look at plots.
* Dinosaurs?
    ```{r,eval=FALSE}
    launchApp("datasaurus")
    aboutApp("datasaurus")
    ```

---

# Nonlinear Relationships in Data?

* We can accomodate non-linear relationships in regressions.

* We'd just add a higher order term like this:
    $$
    y_i = b_0 + b_1 x_i + b_2 x_i^2 + e_i
    $$
    
* This is *multiple regression* (next chapter!)

---

# Nonlinear Relationships in Data?

* For example, suppose we had this data and fit the above regression:
    ```{r non-line-cars-ols2,echo=FALSE,echo=FALSE,fig.height = 5}
    l1 = lm(mpg~hp+I(hp^2),data=mtcars)
    newdata=data.frame(hp=seq(from=min(mtcars$hp),to=max(mtcars$hp),length.out=100))
    newdata$y = predict(l1,newdata=newdata)
    plot(mtcars$hp,mtcars$mpg,xlab="x",ylab="y",pch = 20, cex = 2)
    grid()
    lines(newdata$hp,newdata$y,lw=3,col = "red")
    ```

---
# Analysis of Variance

* Remember that $y_i = \widehat{y}_i + e_i$.

* We have the following decomposition:
    $$\begin{align} Var(y) &= Var(\widehat{y} + e)\\&= Var(\widehat{y}) + Var(e) + 2 Cov(\widehat{y},e)\\&= Var(\widehat{y}) + Var(e)\end{align}$$
    
* Because: $Cov(\hat{y},e)=0$

* Total variation (SST) = Model explained (SSE) + Unexplained (SSR)


---

# Assessing the Goodness of Fit

* The $R^2$ measures how good the model fits the data.

* $R^2$ close to $1$ indicates a very high explanatory power of the model, $R^2$ close to $0$ means that the variations in the outcome $(y)$ are very poorly captured.
    $$
    R^2 = \frac{\text{variance explained}}{\text{total variance}} =     \frac{SSE}{SST} = 1 - \frac{SSR}{SST}\in[0,1]
    $$
    
* NB: Small $R^2$ doesn't mean it's a useless model!

---

# An Example - Log Wage Equation

Let's consider the following example concerning wage data collected in the [1976 Current Population Survey](https://www.census.gov/programs-surveys/cps/library.1976.html) in the USA. 

We want to investigate the relationship between average hourly earnings, and years of education. 

* The data come from the `wooldridge` package. Load it into your global environment (`.GlobalEnv`) with the function `data()`.

```{r echo = FALSE}
data("wage1", package = "wooldridge")   # load data

# a function that returns a plot
plotfun <- function(wage1,log=FALSE,rug = TRUE){
    y = wage1$wage
    if (log){
        y = log(wage1$wage)
    }
    plot(y = y,
       x = wage1$educ, 
       col = "red", pch = 21, bg = "grey",     
       cex=1.25, xaxt="n", frame = FALSE,      # set default x-axis to none
       main = ifelse(log,"log(Wages) vs. Education, 1976","Wages vs. Education, 1976"),
       xlab = "years of education", 
       ylab = ifelse(log,"Log Hourly wages","Hourly wages"))
    axis(side = 1, at = c(0,6,12,18))         # add custom ticks to x axis
    if (rug) rug(wage1$wage, side=2, col="red")        # add `rug` to y axis
}
```

* Briefly describe the data and produce a plot to vizualize the link between `wage` and `education`.
--
```{r echo = FALSE, fig.height = 3, fig.width=10}
par(mfcol = c(1,2))  # set up a plot with 2 panels
# plot 1: standard scatter plot
plotfun(wage1)

# plot 2: add a panel with histogram+density
hist(wage1$wage,prob = TRUE, col = "grey", border = "red", 
     main = "Histogram of wages and Density",xlab = "hourly wage")
lines(density(wage1$wage), col = "black", lw = 2)
```
---

# An Example - Log Wage Equation

OLS regression : let's regress the wage on the education. For this you have to use the `lm()` function.

--

```{r}
my_reg = lm(wage ~ educ, data = wage1)
```

--

We can add the resulting regression line to our last plot

.pull-left[

```{r, eval = FALSE, out.width=3}
plotfun(wage1)
abline(my_reg, col = 'black', lw = 2) # add regression line
```
]

--

.pull-right[
```{r echo = FALSE, fig.height=3.5, fig.width=6}
plotfun(wage1)
abline(my_reg, col = 'black', lw = 2) # add regression line
```

]


---
class: inverse

# An Example - Log Wage Equation : Task 

**Questions : Interpretation**

Here is the summary of the OLS regression. 

.pull-left[

```{r}
summary(my_reg)
```
]

--

.pull-right[

1) With zero year of education, the hourly wage is about **??** dollars per hour.

2) Each additional year of education increase hourly wage by **??** cents.

3) Compute the predicted wage associated to 15 years of education.
]

---
class: inverse 

# An Example - Log Wage Equation : Task

**Answers** 

Answer 1) -0.9 dollars per hour (row named `(Intercept)`)

--

Answer 2) Each additional year of education increase hourly wage by 54 cents. (row named `educ`)

--

Answer 3) We predict roughly **-0.9** + 0.541 * 15 = 7.215 dollars/h.

---

class: title-slide-final, middle
background-image: url(../img/logo/ScPo-econ.png)
background-size: 250px
background-position: 9% 19%

# END




|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="mailto:pierre.villedieu@sciencespo.fr">.ScPored[<i class="fa fa-paper-plane fa-fw"></i>]               | pierre.villedieu@sciencespo.fr       |
| <a href="https://github.com/ScPoEcon/ScPoEconometrics-Slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="https://scpoecon.github.io/ScPoEconometrics">.ScPored[<i class="fa fa-link fa-fw"></i>] | Book |
| <a href="http://twitter.com/ScPoEcon">.ScPored[<i class="fa fa-twitter fa-fw"></i>]                          | @ScPoEcon                         |
| <a href="http://github.com/ScPoEcon">.ScPored[<i class="fa fa-github fa-fw"></i>]                          | @ScPoEcon                       |

